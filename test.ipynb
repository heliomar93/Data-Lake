{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23bb2d-cf1d-4834-9a82-95e32c5ca7c7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fbe9800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:58:21.536676Z",
     "iopub.status.busy": "2022-09-11T18:58:21.536246Z",
     "iopub.status.idle": "2022-09-11T18:58:21.875737Z",
     "shell.execute_reply": "2022-09-11T18:58:21.874808Z",
     "shell.execute_reply.started": "2022-09-11T18:58:21.536644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ade36106c34adba9268b8db2e97672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import udf, col,year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af9d44-cbf5-4ddb-a4bd-10848eb7a7c0",
   "metadata": {},
   "source": [
    "# Song data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9174fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T17:46:17.102207Z",
     "iopub.status.busy": "2022-09-11T17:46:17.099898Z",
     "iopub.status.idle": "2022-09-11T18:01:52.137399Z",
     "shell.execute_reply": "2022-09-11T18:01:52.135805Z",
     "shell.execute_reply.started": "2022-09-11T17:46:17.102065Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b783e1514ce744888064efd95ecb01a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Song data - larger sample\n",
    "path_song_data_g = 's3a://udacity-dend/song_data/A/*/*/*.json'\n",
    "df_song_g = spark.read.json(path_song_data_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c440f1b-ae7f-4037-88df-cc5984615d4c",
   "metadata": {},
   "source": [
    "# Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497c3563-2128-44d0-91eb-a0b6fab199e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:02:44.650238Z",
     "iopub.status.busy": "2022-09-11T18:02:44.649773Z",
     "iopub.status.idle": "2022-09-11T18:02:52.664876Z",
     "shell.execute_reply": "2022-09-11T18:02:52.663558Z",
     "shell.execute_reply.started": "2022-09-11T18:02:44.650197Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54196778c7864a26927d93b0d569ac73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log data - larger sample\n",
    "path_log_data_g = 's3a://udacity-dend/log_data/2018/*/*.json'\n",
    "df_log_g = spark.read.json(path_log_data_g)\n",
    "df = df_log_g.filter(df_log_g.page == \"NextSong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0a3759c-e221-40c3-9ab1-4b6f0874386a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T18:06:39.197586Z",
     "iopub.status.busy": "2022-09-11T18:06:39.197147Z",
     "iopub.status.idle": "2022-09-11T18:06:39.569900Z",
     "shell.execute_reply": "2022-09-11T18:06:39.566432Z",
     "shell.execute_reply.started": "2022-09-11T18:06:39.197549Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeb55cc46424df29af6a841b3153c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_log_g.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8be9f832-3422-4a35-9175-2c3c11495a4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:10:37.289279Z",
     "iopub.status.busy": "2022-09-11T18:10:37.288948Z",
     "iopub.status.idle": "2022-09-11T18:10:37.664720Z",
     "shell.execute_reply": "2022-09-11T18:10:37.663615Z",
     "shell.execute_reply.started": "2022-09-11T18:10:37.289251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832bdc456cc240a6bc358715ffc657cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df_log_g.filter(df_log_g.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e681d420-1468-4f94-bd71-4c265dea3b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:10:54.053731Z",
     "iopub.status.busy": "2022-09-11T18:10:54.053381Z",
     "iopub.status.idle": "2022-09-11T18:10:54.448128Z",
     "shell.execute_reply": "2022-09-11T18:10:54.446986Z",
     "shell.execute_reply.started": "2022-09-11T18:10:54.053687Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4c7fc2ecad40f6876422add563264b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_timestamp = F.udf(lambda x: datetime.fromtimestamp( (x/1000) ), T.TimestampType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea7ec380-e0fe-4419-863d-8ddb5f46ce28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:35:45.099641Z",
     "iopub.status.busy": "2022-09-11T18:35:45.099157Z",
     "iopub.status.idle": "2022-09-11T18:35:45.672638Z",
     "shell.execute_reply": "2022-09-11T18:35:45.671429Z",
     "shell.execute_reply.started": "2022-09-11T18:35:45.099596Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0ca3433232433c9dd2d4ec411b5677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songplays_table = df.join(df_song_g, (df.song == df_song_g.title) & (df.artist == df_song_g.artist_name) & (df.length == df_song_g.duration), 'left_outer')\\\n",
    "        .select(\n",
    "            df.start_time,\n",
    "            col(\"userId\").alias('user_id'),\n",
    "            df.level,\n",
    "            df_song_g.song_id,\n",
    "            df_song_g.artist_id,\n",
    "            col(\"sessionId\").alias(\"session_id\"),\n",
    "            df.location,\n",
    "            col(\"useragent\").alias(\"user_agent\"),\n",
    "            year('start_time').alias('year'),\n",
    "            month('start_time').alias('month')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e365757-511d-40ba-9c0b-0c5074028882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:46:50.649457Z",
     "iopub.status.busy": "2022-09-11T18:46:50.649132Z",
     "iopub.status.idle": "2022-09-11T18:46:51.049291Z",
     "shell.execute_reply": "2022-09-11T18:46:51.047994Z",
     "shell.execute_reply.started": "2022-09-11T18:46:50.649428Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c34b5402894565b2a5ba77aebbf8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e9292af-3a43-4571-8539-c644d7c7d404",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T18:36:37.209579Z",
     "iopub.status.busy": "2022-09-11T18:36:37.209176Z",
     "iopub.status.idle": "2022-09-11T18:36:51.163124Z",
     "shell.execute_reply": "2022-09-11T18:36:51.161379Z",
     "shell.execute_reply.started": "2022-09-11T18:36:37.209547Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9628ff7ad11c432785d14d14288a0f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o795.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 12.0 failed 4 times, most recent failure: Lost task 7.3 in stage 12.0 (TID 22410, ip-172-31-28-203.ec2.internal, executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 841, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o795.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 12.0 failed 4 times, most recent failure: Lost task 7.3 in stage 12.0 (TID 22410, ip-172-31-28-203.ec2.internal, executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001277/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.write.parquet('./Results/' + 'songplays_table/' + 'songplays.parquet',partitionBy=['start_time', 'user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b04dec28-ad75-465c-8d44-1ff8bd25455e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:48:35.213665Z",
     "iopub.status.busy": "2022-09-11T18:48:35.213337Z",
     "iopub.status.idle": "2022-09-11T18:48:35.550953Z",
     "shell.execute_reply": "2022-09-11T18:48:35.549581Z",
     "shell.execute_reply.started": "2022-09-11T18:48:35.213636Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9342a3c411d4b39be380c57ce0bba14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11cb6c30-c1ba-4994-8cfe-240b323ecca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:46:03.149266Z",
     "iopub.status.busy": "2022-09-11T18:46:03.148926Z",
     "iopub.status.idle": "2022-09-11T18:46:03.549060Z",
     "shell.execute_reply": "2022-09-11T18:46:03.547952Z",
     "shell.execute_reply.started": "2022-09-11T18:46:03.149236Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ef40f6fff849ed916dec80ca8668ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'SparkSession' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'SparkSession' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "  .builder\\\n",
    "  .appName(\"Your App\")\\\n",
    "  .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e1e8df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T18:51:02.091633Z",
     "iopub.status.busy": "2022-09-11T18:51:02.091192Z",
     "iopub.status.idle": "2022-09-11T18:51:18.250260Z",
     "shell.execute_reply": "2022-09-11T18:51:18.243212Z",
     "shell.execute_reply.started": "2022-09-11T18:51:02.091601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737b937d039d44d3a5fc0a581604e3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o1073.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 16.0 failed 4 times, most recent failure: Lost task 7.3 in stage 16.0 (TID 22507, ip-172-31-28-203.ec2.internal, executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 841, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1073.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 16.0 failed 4 times, most recent failure: Lost task 7.3 in stage 16.0 (TID 22507, ip-172-31-28-203.ec2.internal, executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n",
      "\t... 33 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 372, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 367, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/worker.py\", line 78, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1662917449746_0002/container_1662917449746_0002_01_001282/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'datetime' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:131)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:220)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:348)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.write.parquet('s3a://testeheliomardata/' + 'songplays/' + 'songplays.parquet',partitionBy=['start_time', 'user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237ea9e-ddb3-4100-a199-e54ad00a9378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
